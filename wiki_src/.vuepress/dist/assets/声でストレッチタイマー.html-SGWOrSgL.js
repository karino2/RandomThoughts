import{_ as a,c as r,a as o,o as n}from"./app-CHiHTJW6.js";const p={};function t(i,e){return n(),r("div",null,[...e[0]||(e[0]=[o('<p>[[そのうちやりたい事]]の一つ。</p><p>[[ストレッチ]]してて、30秒タイマーを音声で始めたい。 録音にして音声っぽいの来たら送る感じにしたいが、この形態だとどこかのクラウドサービスに送らないとだめそうなので無料枠でイケるかは調査がいるなぁ。</p><h2 id="cloudでの音声認識" tabindex="-1"><a class="header-anchor" href="#cloudでの音声認識"><span>Cloudでの音声認識</span></a></h2><p>Googleは1時間まで無料、Azureは5音声時間まで無料。Azureはしばらくは十分かもしれない。</p><p><a href="https://stackoverflow.com/questions/39765018/android-record-audio-while-doing-speech-recognition" target="_blank" rel="noopener noreferrer">Android record audio while doing speech recognition - Stack Overflow</a></p><p>同じようなことをやろうとしてる話。</p><p><a href="https://github.com/GoogleCloudPlatform/android-docs-samples/tree/master/speech/Speech" target="_blank" rel="noopener noreferrer">android-docs-samples/speech/Speech at master · GoogleCloudPlatform/android-docs-samples</a> Cloud APIのAndroidの例。</p><p>認証周りが面倒そうだなぁ。</p><h2 id="pytorch-mobileでのオフライン認識" tabindex="-1"><a class="header-anchor" href="#pytorch-mobileでのオフライン認識"><span>PyTorch Mobileでのオフライン認識</span></a></h2><p>TFLiteでは日本語の音声認識のモデルは無さそう。 PyTorch Mobileというのはあって、これがどのくらい動くのかを試したい気がする。</p><ul><li><a href="https://pytorch.org/mobile/android/" target="_blank" rel="noopener noreferrer">Android - PyTorch</a></li><li><a href="https://github.com/pytorch/android-demo-app/tree/master/StreamingASR" target="_blank" rel="noopener noreferrer">android-demo-app/StreamingASR at master · pytorch/android-demo-app</a></li></ul><h2 id="作業ログ" tabindex="-1"><a class="header-anchor" href="#作業ログ"><span>作業ログ</span></a></h2><p>とりあえず作業した事をメモしておく。(2022/08/15)</p><h3 id="デモのstreamingasrを試してみる" tabindex="-1"><a class="header-anchor" href="#デモのstreamingasrを試してみる"><span>デモのStreamingASRを試してみる</span></a></h3><p>とりあえずは提供されているモデルをそのまま動かしてみる。</p><p>git cloneして、streaming_asrv2.ptlをダウンロードしassets下に置く。</p><p>あまり精度は良くないがとりあえず動いた。こういうのが一発で動くのは体験がいいな。Googleとの技術力の違いを感じるよなぁ。</p><p>WRITE_EXTERNAL_STORAGEとかのpermission、コードを見る限りは要らなさそうだけどいるのかしら？ 消してみよう。＞動いた</p><h3 id="デモのspeechrecognitionの方を動かしてみる" tabindex="-1"><a class="header-anchor" href="#デモのspeechrecognitionの方を動かしてみる"><span>デモのSpeechRecognitionの方を動かしてみる</span></a></h3><p>次は日本語モデルを使いたいので、自前でコンバートを調べたい。 なんかstreamingはモデルのインターフェースを変更する作業が必要そうだな。 これはどうなんだろう。</p><p>それよりは音の区切りは自分でハンドルして、単発の単語を送る、という風にしたい気がする。 SpeechRecognitionの方のサンプルも動かしてみよう。</p><p>なんかこっちはNDKが必要と言われるな。何に使ってるのかしら？</p><p>動かしたら精度はだいぶ高いな。ただ3秒くらい認識にかかっている気がする。少し遅いが、まぁこれでやってみるかなぁ。</p><h3 id="日本語のモデルを試すための調査" tabindex="-1"><a class="header-anchor" href="#日本語のモデルを試すための調査"><span>日本語のモデルを試すための調査</span></a></h3><p>モデルとしては以下を試すか。 <a href="https://huggingface.co/NTQAI/wav2vec2-large-japanese" target="_blank" rel="noopener noreferrer">NTQAI/wav2vec2-large-japanese · Hugging Face</a></p><p>SpeachRegonitionの変換コードを見ていると、logitからローマ字への変換は手動でやっているな。 これって日本語だとWav2Vec2Processorが必要な気がするんだけど、モバイルではどうするのがいいんだろうか？ いまいちやり方は載ってないな。</p><p>変換する方法もありそうな気もするけれど、所詮logitからなにかへの変換テーブルなんだろうから、pythonからkotlinのテーブル吐き出させればいいか？ なんかdexの制限に当たりそうな気もするか。 とりあえずjitを試してみて素直に動きそうならそれで、無理そうなら変換テーブルを作る方向でいこう。</p><h3 id="日本語モデルのコンバート" tabindex="-1"><a class="header-anchor" href="#日本語モデルのコンバート"><span>日本語モデルのコンバート</span></a></h3><p>まずはSpeechRecognitionのデモアプリのコンバートを自力でも試す。 colabで出来ないかな。</p><p>出来た。pytorch_android_liteのバージョンを上げたらロード出来て実行出来た。</p><p>以後以下のcolabで作業する。</p><p><a href="https://colab.research.google.com/drive/1W1ICWH4AzrUFEpEgpVN4VbvvkvJkRwZu#scrollTo=E8Fj4HarKjHn" target="_blank" rel="noopener noreferrer">wave2vec_conv.ipynb - Colaboratory</a></p><p>pred_idはvocab.jsonに書いてあるものっぽいな。まぁtokenizerから取り出してモデルに持たせるか。</p><p>出来た。でも精度はいまいちだな。この辺に特化したfine tuneしないとだめか。</p><h3 id="ひらがなモデルの評価" tabindex="-1"><a class="header-anchor" href="#ひらがなモデルの評価"><span>ひらがなモデルの評価</span></a></h3><p>ひらがなモデルというのがあった。こっちの方が使いやすいのでは？と思い評価してみる。</p><p><a href="https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese-hiragana" target="_blank" rel="noopener noreferrer">vumichien/wav2vec2-large-xlsr-japanese-hiragana · Hugging Face</a></p><p>うーむ、こちらの方が良いが、それでも実用には遠いか。</p><h3 id="英語のlv60-selfのモデル" tabindex="-1"><a class="header-anchor" href="#英語のlv60-selfのモデル"><span>英語のlv60 selfのモデル</span></a></h3><p>&quot;facebook/wav2vec2-large-960h-lv60-self&quot;はbaseよりスコアが良いという事に気づいて試してみる。 これは単語レベルでは割と使い物になる。</p><h3 id="ただ一定以上の音だったら始めれば良いだけでは" tabindex="-1"><a class="header-anchor" href="#ただ一定以上の音だったら始めれば良いだけでは"><span>ただ一定以上の音だったら始めれば良いだけでは？</span></a></h3><p>しばらく音声認識を試していて、結局やりたい事は開始だけなのだから、音量だけでいいんじゃないか？ という気がしてくる。</p>',42)])])}const c=a(p,[["render",t]]),h=JSON.parse('{"path":"/%E5%A3%B0%E3%81%A7%E3%82%B9%E3%83%88%E3%83%AC%E3%83%83%E3%83%81%E3%82%BF%E3%82%A4%E3%83%9E%E3%83%BC.html","title":"","lang":"en-US","frontmatter":{},"git":{"updatedTime":1660736730000,"contributors":[{"name":"Kazuma Arino","username":"","email":"hogeika2@gmail.com","commits":1}],"changelog":[{"hash":"f2923c32c221e8586f3f96ff2cf149172a38337f","time":1660736730000,"email":"hogeika2@gmail.com","author":"Kazuma Arino","message":"update"}]},"filePathRelative":"声でストレッチタイマー.md"}');export{c as comp,h as data};
