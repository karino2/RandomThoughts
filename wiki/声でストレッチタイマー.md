[そのうちやりたい事](そのうちやりたい事.md)の一つ。

[ストレッチ](ストレッチ.md)してて、30秒タイマーを音声で始めたい。
録音にして音声っぽいの来たら送る感じにしたいが、この形態だとどこかのクラウドサービスに送らないとだめそうなので無料枠でイケるかは調査がいるなぁ。

## Cloudでの音声認識

Googleは1時間まで無料、Azureは5音声時間まで無料。Azureはしばらくは十分かもしれない。

[Android record audio while doing speech recognition - Stack Overflow](https://stackoverflow.com/questions/39765018/android-record-audio-while-doing-speech-recognition)

同じようなことをやろうとしてる話。

[android-docs-samples/speech/Speech at master · GoogleCloudPlatform/android-docs-samples](https://github.com/GoogleCloudPlatform/android-docs-samples/tree/master/speech/Speech) Cloud APIのAndroidの例。

認証周りが面倒そうだなぁ。

## PyTorch Mobileでのオフライン認識

TFLiteでは日本語の音声認識のモデルは無さそう。
PyTorch Mobileというのはあって、これがどのくらい動くのかを試したい気がする。

- [Android - PyTorch](https://pytorch.org/mobile/android/)
- [android-demo-app/StreamingASR at master · pytorch/android-demo-app](https://github.com/pytorch/android-demo-app/tree/master/StreamingASR)

## 作業ログ

とりあえず作業した事をメモしておく。(2022/08/15)

### デモのStreamingASRを試してみる

とりあえずは提供されているモデルをそのまま動かしてみる。

git cloneして、streaming_asrv2.ptlをダウンロードしassets下に置く。

あまり精度は良くないがとりあえず動いた。こういうのが一発で動くのは体験がいいな。Googleとの技術力の違いを感じるよなぁ。

WRITE_EXTERNAL_STORAGEとかのpermission、コードを見る限りは要らなさそうだけどいるのかしら？
消してみよう。＞動いた

### デモのSpeechRecognitionの方を動かしてみる

次は日本語モデルを使いたいので、自前でコンバートを調べたい。
なんかstreamingはモデルのインターフェースを変更する作業が必要そうだな。
これはどうなんだろう。

それよりは音の区切りは自分でハンドルして、単発の単語を送る、という風にしたい気がする。
SpeechRecognitionの方のサンプルも動かしてみよう。

なんかこっちはNDKが必要と言われるな。何に使ってるのかしら？

動かしたら精度はだいぶ高いな。ただ3秒くらい認識にかかっている気がする。少し遅いが、まぁこれでやってみるかなぁ。

### 日本語のモデルを試すための調査

モデルとしては以下を試すか。 [NTQAI/wav2vec2-large-japanese · Hugging Face](https://huggingface.co/NTQAI/wav2vec2-large-japanese)

SpeachRegonitionの変換コードを見ていると、logitからローマ字への変換は手動でやっているな。
これって日本語だとWav2Vec2Processorが必要な気がするんだけど、モバイルではどうするのがいいんだろうか？
いまいちやり方は載ってないな。

変換する方法もありそうな気もするけれど、所詮logitからなにかへの変換テーブルなんだろうから、pythonからkotlinのテーブル吐き出させればいいか？
なんかdexの制限に当たりそうな気もするか。
とりあえずjitを試してみて素直に動きそうならそれで、無理そうなら変換テーブルを作る方向でいこう。

### 日本語モデルのコンバート

まずはSpeechRecognitionのデモアプリのコンバートを自力でも試す。
colabで出来ないかな。

出来た。pytorch_android_liteのバージョンを上げたらロード出来て実行出来た。

以後以下のcolabで作業する。

[wave2vec_conv.ipynb - Colaboratory](https://colab.research.google.com/drive/1W1ICWH4AzrUFEpEgpVN4VbvvkvJkRwZu#scrollTo=E8Fj4HarKjHn)

pred_idはvocab.jsonに書いてあるものっぽいな。まぁtokenizerから取り出してモデルに持たせるか。

出来た。でも精度はいまいちだな。この辺に特化したfine tuneしないとだめか。

### ひらがなモデルの評価

ひらがなモデルというのがあった。こっちの方が使いやすいのでは？と思い評価してみる。

[vumichien/wav2vec2-large-xlsr-japanese-hiragana · Hugging Face](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese-hiragana)

うーむ、こちらの方が良いが、それでも実用には遠いか。

### 英語のlv60 selfのモデル

"facebook/wav2vec2-large-960h-lv60-self"はbaseよりスコアが良いという事に気づいて試してみる。
これは単語レベルでは割と使い物になる。

### ただ一定以上の音だったら始めれば良いだけでは？

しばらく音声認識を試していて、結局やりたい事は開始だけなのだから、音量だけでいいんじゃないか？
という気がしてくる。